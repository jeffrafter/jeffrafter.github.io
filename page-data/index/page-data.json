{"componentChunkName":"component---src-pages-index-tsx","path":"/","result":{"data":{"site":{"siteMetadata":{"title":"Jeff Rafter","keywords":["jeffrafter","how-to","blog"]}},"allMarkdownRemark":{"edges":[{"node":{"excerpt":"After getting started with Unity and Virtual Reality, you now have a world. You’ve made it multiplayer with Normcore and are hanging out with your friends. To turn this into a game you need to introduce hits and damage. Preamble: The implementation of damage in this post is not perfect. There are a lot of tradeoffs in multiplayer games where you need to decide if you want to trust the client or the server. This post is a starting point for understanding how to implement hits and damage in Unity. In this post we will trust the client and skip validation on the server (which should be done in a real game). We will also attempt to allow the owner to control their own health (which is a security risk in a real game) and simulate the physics of projectiles. It feels the best and works well for a small group of friends playing together.  Ownership As we’ve mentioned before in Unity Multiplayer VR with Normcore, we need to understand ownership. In Normcore, all shared objects have a  and movable objects have a .Generally these objects have a single owner who is allowed to update their values. All other clients receive updates from the owner and interpolate between them. This is important because we want to allow the owner to control their own health. We’ll create an  model that will be owned by the player and will be responsible for updating their health. This model will be shared with all clients and will be responsible for updating the health of the player. The problem comes about when I try to shoot you. I create a projectile (like a bullet or arrow) and send it to you. I’ll be updating the Realtime Position of the projectile on my client and you’ll be receiving those updates and interpolating any gaps. If my projectile hits you, I am not allowed to update your health. I can only update my own health. So how do we handle this? One option is to allow you to check if the projectile hits you on each frame. Unfortunately this is not very efficient and can lead to a lot of false misses. Suppose for example that I shoot you and the projectile is moving very fast. If you only check once per frame, you might miss the projectile entirely. If you check more frequently, you might hit the projectile multiple times. This is not ideal. Instead we’ll use a  model that will be owned by the player that is hit, but created by the player that is shooting. This model will be responsible for hitting player and updating their health.  Attributes and synchronization  Firing…","fields":{"slug":"/hits-and-damage-in-multiplayer-games-in-unity/"},"frontmatter":{"excerpt":"After getting started with Unity and Virtual Reality, you now have a world. You've made it multiplayer with Normcore and are hanging out with your friends. To turn this into a game you need to introduce hits and damage.","date":"October 04, 2024","title":"Hits and Damage in Multiplayer Games in Unity"}}},{"node":{"excerpt":"Just use docker, they said. It will be easy, they said. I’ve been trying to get my iPad to print to a PDF for a while now. I’ve used PDFWriter but I wasn’t able to print from an iOS simulator on my localhost. Realized that I could use CUPS-PDF to print to a PDF and then use Avahi to broadcast it as an AirPrint printer and that this might help me workaround the DNS rebinding protection built into CUPS. Spoiler: it didn’t. But this was a fun little project and I learned a lot about Docker, CUPS-PDF and Avahi. Getting started CUPS-PDF is a virtual printer that allows you to print to a PDF. It’s a great way to create PDFs from any application that supports printing. You can use it to create PDFs from web pages, documents, images, etc. It runs in Linux and relies on the CUPS printing system. In our previous article, Printers, Fake Printers, and AirPrint we used the Mac-specific RWTS-PDFWriter as a virtual printer. The setup was simple and it worked well. Unfortunately, RWTS-PDFWriter relies on the underlying system installation of CUPS. Because of this it no longer supports PostScript (as of MacOS Sonoma) and isn’t using the latest version of CUPS based on the OpenPrinting project. Avahi is a zeroconf implementation that allows you to broadcast services on a local network. It’s a great way to discover services on your local network without having to configure DNS or DHCP. It’s used by Apple’s Bonjour protocol to broadcast services like AirPrint. Docker is a containerization platform that allows you to run applications in a consistent environment. It’s a great way to run applications without having to worry about dependencies or conflicts with other applications. On macOS, Docker runs in a hypervisor environment and you can use it to run Linux applications (note, it is not an actual virtual machine). In order to run Docker on macOS, you need to install Docker Desktop. This will install the Docker CLI, Docker Compose and the Docker Engine. You can then use the Docker CLI to run containers, build images and manage your Docker environment. Go to https://www.docker.com/products/docker-desktop/ and download Docker Desktop for Mac.  Setting up the Docker container Docker containers rely on  files. Create a new folder and add a  with the following content: Notice we are expecting a  script. Create a new file called  in the same folder as your  with the following content: This script stops any service that auto-started, starts the CUPS daemon in the background, waits…","fields":{"slug":"/docker-and-cups-pdf/"},"frontmatter":{"excerpt":"Just use docker to print to a PDF from your iPad ™️","date":"February 17, 2024","title":"Docker, CUPS-PDF, Avahi and AirPrint"}}},{"node":{"excerpt":"When the economy crashes and the world ends you’ll find us all trading ink cartridges for currency. Until then, you probably want to make amazing prints of your favorite projects and if you are like me you are afraid to spend the ink. When I was working on my PhotoBooth project I found myself iterating on the print design. I’d done the math and figured out that each print on my photo printer would cost me just under $1.00. I was not going to print 1000 copies of my design to get it right. I needed a way to print to a PDF to test my designs. Instead of solving the problem in code (which I show at the end of this post) I found myself solving it in the printer. I had to figure out how to print to a PDF from an iPad. If I could start over, I would solve the problem in code and just render the PDF and send it to a file.  Fake printers are like real printers My real printer connects to my WiFi (after an incredible number of strange key-presses) and has a Bonjour name. I can print to it from my phone, my iPad, and my laptop. I can also print to it from my server. I can print to it from anywhere on my network. Bonjour is Apple’s implementation of Zeroconf or multicast DNS. It’s a way for devices to find each other on a network, including printers, and advertise their services. Now, all I needed was a fake printer I could install onto my Mac. It turns out PDFwriter is exactly that. It’s a printer driver that prints to a PDF. Download the package and install it.  Then run the PDFWriter Utility to create the destination folder for your PDFs. The utility will create a folder in . And then it will create a symlink to that folder at . It will also create a folder  (or  depending on your version). Technically, it will not create these immediately but on first use.    You’re done. You can now print to a PDF from any application on your Mac. Select  as your printer and you will see the PDF show up in the destination folder you created. But we want to be able to print to it from our iPad. Open your Printer Sharing settings and share the PDFwriter printer.  This makes the printer available to other computers on your network, but not on your iPad. It turns out this is exactly what would happen with a real printer. At this point you should restart your computer. This will ensure that the printer is properly shared and that the CUPS configuration is updated. It is possible that you don’t need to do this, but I couldn’t find the magic set of services that needed to be restarted…","fields":{"slug":"/printers-fake-printers-and-airprint/"},"frontmatter":{"excerpt":"When the economy crashes and the world ends you'll find us all trading ink cartridges for currency.","date":"February 16, 2024","title":"Printers, Fake Printers, and AirPrint"}}},{"node":{"excerpt":"What’s better than Virtual Reality? Virtual Reality with friends. Even the most basic games become more fun when played together. Previously, we focused on getting started with Unity and Virtual Reality. This post builds on top of that foundation: adding multiplayer support, multi-platform support, multiple controllers, and virtual reality avatars. Dance the macarena  Choosing a multiplayer package There are a number of different approaches for building multiplayer games in Unity, some of the most common are: Mirror (Unity multiplayer networking) https://github.com/vis2k/Mirror Normcore (Unity multiplayer service) https://normcore.io Photon (Unity multiplayer service) https://www.photonengine.com/ Mirror is open source and fantastic for peer to peer multiplayer. It requires that you (or a player) host the server which requires more work at scale. If you are looking to use Mirror I highly recommend the excellent YouTube series How To Make A Multiplayer Game In Unity - Client-Server - Mirror Networking by DapperDino. Photon and Normcore are hosted services but both have free plans. Photon tends to charge based on the number of concurrently connected users and Normcore based on the hours spent in shared “rooms”. I tend to use Normcore because the kinds of games I want to make work better with their plans and I find it the simplest solution. Depending on your use case you might choose another option. Check out the pricing: https://normcore.io/pricing https://www.photonengine.com/en-US/PUN/Pricing  Planning out the scene Before we start building our scene it is important to outline our goals: Users should be able to view and control their player with a VR headset, Keyboard and AR Each player will be represented by an avatar within the game VR controllers can represent hands and teleport Avatars are rigged with inverse kinematics Networked objects should be sharable between players User control Within our basic example we were able to control the player’s view (what is displayed on the each lens) by moving the camera when the headset moved. In this way, the locomotion of our headset is our main “controller.” But what about our friends that don’t have an Oculus Quest? It would be great if they could also join our multiplayer game. To support this we will create three sets of controls: Virtual reality - the headset and the two VR controllers Augmented reality - using a phone as an AR controller Computer - using a keyboard and mouse to control the player This adds…","fields":{"slug":"/unity-multiplayer-vr-with-normcore/"},"frontmatter":{"excerpt":"What's better than Virtual Reality? Virtual Reality with friends. Even the most basic games become more fun when played together. Learn how to quickly setup a multiplayer VR game with Normcore and Unity.","date":"October 16, 2021","title":"Unity Multiplayer Virtual Reality with Normcore"}}},{"node":{"excerpt":"Virtual reality is incredibly immersive and a blast to play with. The Oculus Quest 2 has made it more accessible than ever before. Of course, the moment I put it on, I immediately wanted to make my own games and got started with Unity. Best practices for setting up your environment and building a game have changed frequently enough that it can be hard to find current tutorials and examples without getting confused. Mac versus Windows If you are using Windows, you are in luck: many of the tutorials and videos you’ll find assume you’re working on Windows. I’m using a MacBook Pro. This can create some challenges but the biggest challenges are around the platform support for the Oculus Quest itself. Oculus doesn’t make a version of it’s Oculus Desktop app (or libraries) available on MacOS. Because of this you’ll usually want to start with tutorials that are specific to the Mac and adjust. This post will try to focus on what works in both environments.  Getting started We’ll be using Unity to develop our game. Unity is constantly releasing new versions and new features. Because of this it is very common to have multiple versions of Unity installed (for different projects). Luckily this is easy to manage using Unity Hub. Download Unity Hub and install it. Once you have installed Unity Hub, you’ll need to install a version of Unity.  I generally try to install the latest Official Release. Even though the LTS (long term support) versions are guaranteed to receive updates they won’t necessarily have all of the newest features. Choose a version and click the “NEXT” button. When building a game for Oculus Quest (which uses a custom Android operating system internally), you’ll need to use a version of Unity with Android Support and OpenJDK. Select the Android Build Support option (and all sub-options):  I also select  and . If you’re not sure if you’ll need something, you can always add modules later. Click the “NEXT” button. You’ll need to agree to the Android SDK and NDK License Terms from Google:  Check the box and click the “DONE” button. The installation should take a few minutes.  Setting up a basic project In Unity Hub, click on Projects on the left, then click on the New button. Create a new project with the Universal Render Pipeline selected:  The project should open and have a default scene in it. If you want to use this default construction scene that is okay (and in some cases more interesting) - it simply takes longer to build the scene and recompile. For…","fields":{"slug":"/getting-started-with-unity-and-vr/"},"frontmatter":{"excerpt":"Virtual reality is incredibly immersive and a blast to play with. The Oculus Quest 2 has made it more accessible than ever before. Best practices for setting up your environment and building a game have changed frequently enough that it can be hard to find current tutorials and examples without getting confused.","date":"January 16, 2021","title":"Getting Started With Unity and Virtual Reality"}}},{"node":{"excerpt":"🗺 Exploring - Work in progress Using Express to build out your API can be challenging; but integrating it with all of the products available from Amazon Web Services can feel impossible. This post focuses less on best practices for your API and more on reusable patterns to tie all of those products together. This is the second part in a series on building out a scalable website on AWS. In the first part we setup all of the infrastructure we’ll need using Terraform.  Choosing a framework Before we even get started we need to decide what framework we’ll use to write our AWS Lambda function - if any. Out of the box AWS gives you a  pattern that you can use to receive an incoming web request, perform some actions and return a response. For a web server this is really all we need. In fact, using this simple pattern is actually the recommended best practice. AWS Lambda is a FaaS (Function as a Service) or “serverless” platform designed to execute discrete “functions” or handlers. The expectation is that each route on our website will execute a separate specific function inside Lambda. Each of these routes would be setup individually inside AWS API Gateway and point to their respective AWS Lambda functions. We’ve setup our API Gateway to send all routes to a single function (). Why would we do this? What are the tradeoffs? All routes pointing to a single function When developing your API you want to be able to test it locally When deploying you want shared dependencies to be updated simultaneously Adding a new route to API Gateway and pointing to a new Lambda function is cumbersome Changing API Gateway can be slow Our function will have fewer cold-starts Many routes pointing to separate functions You might want to restrict deployments for specific parts of the site You can change parts of the site without changing everything A single mistake won’t break the entire site Deploying new functions will only cause cold-starts on part of the site You want to use Lambda@Edge Smaller functions take less time to upload and deploy Smaller functions have a shorter cold-start time In general, how you choose between these trade-offs is dictated by the site and team (and in some cases preferences). If you have a smaller site and smaller team then it is often better to favor simplicity. Choosing All routes pointing to a single function simplifies local development and deployment but increases risk by putting everything in one function. For example, if you are the only developer…","fields":{"slug":"/express-and-aws/"},"frontmatter":{"excerpt":"Using Express to build out your API can be challenging; but integrating it with all of the products available from Amazon Web Services can feel impossible.","date":"October 09, 2020","title":"Exploring - Using Express and Amazon Web Services"}}},{"node":{"excerpt":"I’ve previously written about building a static website with Gatsby and TypeScript. If you’re building a static website like a blog then deploying to GitHub Pages is more than enough. But what if the website you are building needs user authentication and authorization? What if you need to store and retrieve items from a database? What if you need server side processing. What if you want to integrate payment processing? There are a lot of options and managing it all quickly becomes difficult. Who is this for? This is a long post. Even by my standards. Also, this isn’t exactly introductory or beginner friendly (though I do try to explain how things are setup and how to make decisions). This post is more of a long-winded reference. I’ve written it because I needed opinionated documentation with all of my choices in one place. Your choices will probably be different because there are lots of trade-offs. Because this is opinionated it is also limited. There are many deeper technical topics I omit because I don’t use them. Nevertheless, hopefully this is useful for you. Some history Years ago, I was helping to build an E-Commerce startup. It was one of those experiences where you’re surrounded by brilliant and creative people that can make anything. Our platform was built using Ruby on Rails 1.2, relied on an early beta of Stripe for payment processing, and we used the beta of Slack to communicate. We deployed everything to Amazon Web Services because we knew it could scale. At the time the tooling was limited. So we hired a consultant named Mitchell Hashimoto to help us. Mitchell and I would work late setting up the infrastructure using a series of YAML files and a AWS API tool called fog. It was rudimentary but it worked amazingly well. From experience I can tell you that Mitchell is one of the fastest, most brilliant people I have ever seen. He built everything we needed flawlessly. One night, he started telling me about what he envisioned as a much better configuration language and platform. Looking back, I realize now he was envisioning Terraform. Setup Before you begin building anything it is useful to understand the trade-offs associated with different platforms, frameworks, and languages. When I’m building something new I tend to optimize for the following: Price Scalability Performance Maintainability It is very likely your priorities are different. I put price first because I don’t want to be restricted from making a lot of websites. Using Now or…","fields":{"slug":"/terraform-and-aws/"},"frontmatter":{"excerpt":"There are a lot of options for configuring AWS. Terraform from HashiCorp is a tool for configuring remote infrastructure. You can create Terraform configuration files and treat your infrastructure as code; versioning the changes and storing your settings in your repository.","date":"October 09, 2020","title":"Terraform and Amazon Web Services"}}},{"node":{"excerpt":"Working with Unity makes development easy. You can quickly prototype a game and show it off. But what if there are multiple developers. Storing your source code on GitHub makes collaboration easy; but there are a lot of tradeoffs when working with larger assets and projects. Unity has built-in collaboration functionality: https://unity.com/unity/features/collaborate Likewise, GitHub has released tools for working with GitHub inside Unity: https://unity.github.com/ In my case, I often use the command-line or GitHub Desktop. Create the project. Within the project we’ll need to setup git: There are a lot of files that we don’t want to track in version control. These files are specific to our machine and editor and can’t be easily shared. We want to ignore them, so create a file called : This  is based on the standard one found at https://github.com/github/gitignore/blob/master/Unity.gitignore.\nNote that we are ignoring:  files. These are specific to OSX. You may want to ignore other files specific to your code editor or operating system (for example if you use Vim). We’ve also ignored the  folder. This folder contains IL2CPP files that may or may not be generated depending on how you are building your project. As the name implied you want to keep a backup of this folder for every release of your game (it helps with debugging user submitting bug reports); but we don’t need it in version control. If you normally put your project files in a subfolder you may want to change how the folders are listed. For example: Here the leading  indicates that only folders matching the names in the root folder should be ignored. But in many cases you’ll want to ignore these files even if they are in a subfolder. To do this, remove the leading : Alternatively, you can leave the file as it is and move the  to the subfolder. We’ll have a lot of large files (audio, textures, models). When changing these files the entire file is changed (not a single line as in text files) Next we’ll need to setup Git LFS (large file storage). On GitHub you get a certain amount of LFS storage and bandwidth for free every month. The limits are per account (not per repository): Every account using Git Large File Storage receives 1 GB of free storage and 1 GB a month of free bandwidth. If the bandwidth and storage quotas are not enough, you can choose to purchase an additional quota for Git LFS.  Additional bandwidth or storage costs money (though it is fairly inexpensive). Because of this, the LFS…","fields":{"slug":"/unity-and-github/"},"frontmatter":{"excerpt":"Working with Unity makes development easy. You can quickly prototype a game and show it off. But what if there are multiple developers. Storing your source code on GitHub makes collaboration easy; but there are a lot of tradeoffs when working with larger assets and projects.","date":"September 13, 2020","title":"Working with Unity and GitHub"}}},{"node":{"excerpt":"🗺 Exploring - Work in progress Getting every ounce of performance out of your game is a dark art. Its study leads to game specific and platform specific shader optimizations, baked lighting, mesh decimation, and horrific tales of endless overdraw. To make it even more difficult, the answers that you’ve learned along the way for every other platform and every other game engine (and their successive rendering pipelines) are probably not applicable. For me, I’m just beginning this journey; struggling like many others to make a fun game in my garage using Unity - adding store-bought assets and clicking checkboxes to turn on beautiful effects in my scene. Luckily there are some tricks and many, many good references I’ve stumbled upon (indexed at the end of this post). This post is mostly about working to make your game render at the required 72 FPS (frames-per-second) with all of the visual effects and tricks - especially when you are GPU-bound. This post does not cover complex animations, physics simulations and will only briefly discuss shadows.  Why is this difficult? Just a quick note: you might be wondering why this is hard? Or why this is different from other platforms? Oculus Quest is an all-in-one device - within the headset is everything you need (and everything you have) to run your game. It is an Android-based device with a Snapdragon 835 chipset and built in Adreno 540 GPU: Oculus Quest has two 1600x1440 OLED screens that run at 72 frames per second. By default, apps render to 1216x1344 eye textures… Internally Oculus Quest runs on a Qualcomm Snapdragon 835 chipset… The Snapdragon 835 has 8 cores, 4 of which are large, fast “gold” cores that run up to 2.3 Ghz. Of the four gold cores, applications have full access to 3 of them, while we reserve the fourth for TimeWarp and other system services. The remaining low-power “silver” cores are reserved for tracking and other system software… The Snapdragon 835 chipset includes an Adreno 540 GPU, which runs at 710 Mhz. This is a “binning” or “tiled” GPU, which means that it cuts the scene up into a grid of tiles and renders each in sequence. Oculus Quest comes with 4 GB of memory, 2.75 GB of which is available for applications. - From https://developer.oculus.com/blog/down-the-rabbit-hole-w-oculus-quest-the-hardware-software/?locale=en_US The key here is that the GPU is a tile-based renderer. Because the chipset is smaller (its a mobile device) there is a limited amount of silicon and power. Because of this…","fields":{"slug":"/performance-tuning-for-mobile-vr/"},"frontmatter":{"excerpt":"Whatever you learned tuning the performance for mobile or for desktop or console games might not apply to mobile VR.","date":"July 09, 2020","title":"Exploring - Performance tuning for mobile VR"}}},{"node":{"excerpt":"I’ve always wanted to be able to work with hardware and put together interesting and complex circuits in the same way I’ve been able to build software. Before I was a teenager my grandfather bought me the now classic Getting Started In Electronics which has been called The Greatest Electronics Book Ever Written. I struggled to understand it and never had the parts to try out the circuits; working instead with pencil and paper and ending up confused. These days it is pretty easy to get lots of parts cheaply to try things out.  Buy some parts To get started it is good to have some electronics to play with. We’ll need: Any Arduino (preferably an ESP8266 based device) Breadboard LED A  resistor Some jumper wires Micro-USB cable These parts are often hard to find individually so I tend to buy them in kits. I purchased the following: ESP8266 NodeMCU: https://www.amazon.com/gp/product/B081CSJV2V ($13.99 pack of 3) Smraza Basic Starter Kit: https://www.amazon.com/Arduino-Starter-Tutorials-Compatible-Mega2560/dp/B01MATM4XF ($19.98) Micro-USB cables: https://www.amazon.com/gp/product/B07V6GZ5GM ($7.99 pack of 5) Wall chargers (2.1A 5V): https://www.amazon.com/gp/product/B082TZZJZR ($9.99 pack of 3) You don’t need to buy multiples (often I buy three of each thing), but I’ve found I want to have multiple projects going at once. You might have micro-USB and wall chargers laying around which should be fine: just make sure they are 5V, ~2A  and that the cables are data cables. The Smraza Basic Starter Kit has a ton of parts - most of which we aren’t going to use here; but the kit is great and allows you to build all kinds of projects in the future.  Setup a circuit \n(designed on https://tinkercad.com) About breadboards Setting up a breadboard like this makes it easy to test simple circuits without needing to solder anything. You can connect different components using jumper wires (normal wires, that you push into the various holes on the breadboard). The breadboard makes it easy to do this because certain holes of the breadboard are connected internally. It is tough to guess how everything is connected if you are looking at a breadboard for the first time.  The breadboard has two halves, each with two “power rails” (red and black in the above diagram). Each row is connected: for example row : , , ,  and  are all connected internally (shown in green above). The gap running down the center of the breadboard separates the two sides.  The , , , ,  slots are all connected and…","fields":{"slug":"/getting-started-with-arduino-and-esp8266/"},"frontmatter":{"excerpt":"Understanding the basics of hardware can open new projects and new possibilities and it is more accessible and inexpensive than ever.","date":"May 20, 2020","title":"Getting started with ESP8266 and Arduino"}}},{"node":{"excerpt":"Last week at GitHub we held a day of learning. It was a fantastic opportunity to dig into things I’ve been wanting to learn. I decided to focus on Bluetooth security. Gavin has been working on a new coding based platform that connects his iPad to a small robot using Bluetooth. He wants a better editor (it uses a MakeCode based block editor but has some usability issues); he generally uses Visual Studio Code. I wondered if it would be possible to write our own client and connect to the robot.  Some background Our interest in Bluetooth hacking began a couple of years ago… with a Furby. Hasbro released the Furby Connect in 2012. The Furby Connect is a dancing, singing robotic toy much like its predecessor. Mad hordes of shoppers weren’t tackling one another in the aisles to purchase the Furby Connect as they had done when the original Furby was released. If they had understood its true potential, however, they might have. Furby is probably the most advanced, well designed, impossibly elegant child’s toy that exists. It might be the key to unlocking the emotion of pair-programming. It might seem like the stuff of nightmares: these small talking robots dancing and singing and clamoring for your attention, but as a developer or user experience designer it is an almost perfect medium. About two years ago a few of us built a pair-coding Furby for a GitHub hack week. The Furby watched as you worked in Atom and offered reactions and ideas. We made a short promotional (and unscripted) video: Pair Coding Furby During that project we focused more on designing an empathetic robot partner and utilized existing bluetooth libraries for controlling a Furby. It worked out great, but involved relatively little hacking as someone else had already reverse-engineered the protocol and connection. I wanted to learn how to do that.  Bluetooth Smart For our purposes, we decided to focus on Bluetooth Low Energy (or Bluetooth Smart), a subset of the Bluetooth 4.0 specification. Because it generally uses lower power, it is commonly used for peripherals (especially battery powered devices). Though you can read the full specification there is a much simpler introduction by Adafruit. Quick glossary: Peripheral devices - things like bluetooth headphones, keyboards, speakers, or in our case a robot - advertise a Generic Access Profile or GAP. Central devices - like your phone or iPad, a computer, or some other controller - scan for peripheral devices. Once a central device finds a…","fields":{"slug":"/bluetooth/"},"frontmatter":{"excerpt":"Bluetooth can be an incredibly secure protocol... but it usually isn't.","date":"April 19, 2020","title":"Bluetooth Hacking"}}},{"node":{"excerpt":"Previously, I posted about Working with GitHub Actions. It covered the basics of setting up a repository, configuring embedded actions and the associated workflows. If you haven’t worked with GitHub Actions before, you might want to read that first. Once you’ve created your amazing GitHub Actions, you’ll want to re-use them and share them with your friends. To do that you’ll want to organize your project a little differently. Also, you’ll want to release your actions with all of the dependencies included so that they run efficiently. In this post we’ll create a repository which contains a GitHub Action - built in TypeScript - and a second repository which will use the action. Before you read this it is important to note: starting with a template will save you a lot of time and setup. Creating actions can be very simple. In this post, however, I am going to work through and explain all of the steps. Included in this post are some of the reasons I’ve chosen one particular setup and skipped another. When getting started with GitHub Actions it is difficult to understand how all of the pieces fit together, or why you might want to create an action for a particular task. Hopefully this post provides some helpful examples. That said, there are probably steps here that you’ve seen before, don’t care about, or just want to skip and that’s okay. In order to follow this, you’ll need a GitHub account. You’ll need Node, Node Version Manager (), and Node Package Manager (). The examples will be in TypeScript. All of the code is available at https://github.com/jeffrafter/honk-action. Honk For our example we’ll create a GitHub Action which listens for new issue comments - if the comment doesn’t contain the word “honk” our action will delete the comment and add a new comment with a picture of the goose from Untitled Goose Game. This isn’t very practical, but it should serve as a good foundation for building actions that respond to comments. Here is the layout for our action: Notice that  and  are both in the root of our repository. In this case our action is not very complex, using only a single file. If you have a complex action which contains many modules it might make sense to to put these into a  or  folder. We’ll dig into each of these files. Setup First you want to create a folder for your project:  Environment As we did in the previous post we’ll be using TypeScript to build our action, which requires Node. Out of the box GitHub supports a few environments for your…","fields":{"slug":"/releasing-github-actions/"},"frontmatter":{"excerpt":"Once you've made some amazing GitHub Actions, you'll want to re-use them and share them with your friends. To do that you'll want to organize your project a little differently, publish and release them.","date":"January 31, 2020","title":"Releasing GitHub Actions"}}},{"node":{"excerpt":"GitHub Actions are still in beta and are changing quickly. But if you are looking to get started the possibilities are endless. This guide is mostly about pointing to documentation and exploring some fun ways to use GitHub Actions. In this post we’ll create a repository which contains a GitHub Action - built in TypeScript - and an associated workflow. In the action we’ll respond to push events and output some logging information. Technically, you don’t need a custom script to accomplish this; you could instead build a very simple workflow which runs  commands. Using a full script will allow us to explore more capabilities of GitHub Actions. We’ll also create an action that automatically responds to, and reacts to, issue comments. Before you read this it is important to note: starting with a template will save you a lot of time and setup. In this post, however, I am going to work through and explain all of the steps. Included in this post are some of the reasons I’ve chosen one particular setup and skipped another. When getting started with GitHub Actions it is difficult to understand how all of the pieces fit together, or why you might want to create and action for a particular task. Hopefully this post provides some helpful examples. That said, there are probably steps here that you’ve seen before, don’t care about, or just want to skip and that’s okay. In order to follow this, you’ll need a GitHub account. Additionally, you’ll need to sign up for the GitHub Actions beta. The examples will be in TypeScript. All of the code (and commits) are availble on GitHub: https://github.com/jeffrafter/example-github-action-typescript Documentation The documentation for GitHub Actions is really good (far more complete than this post) and is good to have on hand. You can learn how to build Actions, Workflows and core concepts; as well as dive deeply on using the toolkit, octokit and handling payloads. Automating your Workflow with GitHub Actions GitHub Package Toolkit Event Types & Payloads Rest API V3 octokit/rest.js Getting started First you want to create a folder for your project: We’ll be using TypeScript to build our action, which requires Node. Out of the box GitHub supports a few environments for your actions to run . There is built-in support for running actions built in JavaScript (using Node). So why did I choose to use TypeScript? It makes development a lot easier by providing compile-time checks and hints in my editor about methods and parameters…","fields":{"slug":"/working-with-github-actions/"},"frontmatter":{"excerpt":"GitHub Actions are still in beta and are changing quickly. But if you are looking to get started the possibilities are endless.","date":"September 19, 2019","title":"Working with GitHub Actions"}}},{"node":{"excerpt":"The Oculus Quest is entirely immersive and a blast to play with. Of course, the moment I put it on, I immediately wanted to make my own games and got started with Unity. There are a few blog posts and videos that helped me on my way.  Getting started To develop for you Oculus you’ll not only need an Oculus account but you may want to setup an Oculus Organiztion (free). Additionally (for my purposes), you’ll need a Unity account. I’m using a personal account (also free). Mac versus Windows I’m using a MacBook Pro. Many of the tutorials and videos you’ll find assume you’re working on Windows. This can create some challenges but the biggest challenges are around the platform support for the Oculus Quest itself. Oculus doesn’t make a version of it’s Oculus Desktop app (or libraries) available on MacOS. Because of this you’ll want to start with tutorials that are specific to the Mac and adjust. Unity & initial setup I followed a couple of tutorials for getting Unity Hub and Unity installed. I recommend: How to get started with Oculus Quest and Unity on macOS How to get started with Oculus Quest development in Unity If you prefer videos:  Learn Once I had the basics working, and could load games I built in Unity on my Quest, I wanted to do more. I found a set of fantastic tutorial videos by Valem, Quentin Valembois and was hooked. As I went through the videos I took notes and (with permission) am posting those here. While watching these videos, you might notice Valem is using the ▶ button to debug. This works because he is actually developing for the Rift in the videos (notice that the controllers are upside-down). In a later video on recreating Slenderman (at 1:47) he explains how to use the ▶ button while building for the Oculus Quest. Unfortunately that won’t work on MacOS because the Oculus plugin is not supported. There are notes on how to get this working at the end of this post.  A note on the Oculus Integration asset The Oculus Integration in the Unity Asset Store solves an amazing amount of problems for you. Unfortunately,\nthe updates are not necessarily backward compatible and it makes following tutorials difficult. Most\nof the tutorials listed here were for the 1.38 version. The current 1.39 version requires a few more steps. After you download and import the kit you’ll restart Unity. You should see a new menu for Oculus:  Choose  | . Choose  |  From your project search, find the newly created  and open it in your editor.\nYou’ll want to change  to  so…","fields":{"slug":"/oculus-quest/"},"frontmatter":{"excerpt":"Everyone wants to talk about consumer VR. I just want to make some games.","date":"July 25, 2019","title":"Developing for the Oculus Quest"}}},{"node":{"excerpt":"Updated: February 2020 Creating a static website involves an almost infinite set of choices. Among these is\nGatsby – a static site framework based on , ,  and\nmany other modern approaches. Gatsby is, in many ways, the JavaScript successor to\nJekyll. I’ve upgraded several sites to Gatsby (including this one) finding\na way to integrate TypeScript as part of the journey. In this post I am going to work through all of the pieces of a default Gatsby site and try to explain them along the way. Included in this post are some of the reasons why I’ve chosen one particular plugin or skipped another. Often – especially when you choose a default Gatsby starter – it is difficult to understand how all of the pieces fit together, or how you might build your own starter template. Hopefully this post provides some helpful examples. Also: the Gatsby documentation is extremely good. There is a\nfantastic tutorial, quick start and some recipes. I’ve relied on those and a host of other blogs when working on this\npost. In order to follow this, you’ll need access to a terminal (or console) and you’ll need Node, Node Version Manager, and git installed. All of the code (and commits) are available on GitHub: https://github.com/example-gatsby-typescript-blog/example-gatsby-typescript-blog.github.io Getting started To get started, we’ll follow the quick start. First, install the Gatsby CLI (command line interface): Next, we’ll want to create a new Gatsby site. In this post we’ll assume that our site is called (rather uninspiringly) . Run the following command (it will generate a new folder called  where you run the command): Change directory to newly created folder : The  command created a project folder and automatically installed our node modules and setup defaults. The project structure should look like: With this, we already have enough to run Gatsby and view our site. Run: This will start Gatsby in  mode. Open a browser to  and you should see:  The default covers the basics but isn’t very personalized. Let’s work on that now.  Configuring the node version Gatsby requires Node to run on your computer. If you have multiple local projects on your computer, you might run into a conflict about which Node version should be used. Node Version Manager solves this problem. After installing a Node Version Manager, check which version of Node you have installed: If you don’t have the version you want, find a remote version: This will show lots of versions. Find the last one with  (for Long…","fields":{"slug":"/gatsby-with-typescript/"},"frontmatter":{"excerpt":"Creating a static website involves an almost infinite set of choices. I've upgraded several sites to Gatsby (including this one) finding a way to integrate TypeScript as part of the journey. Gatsby leverages React, JSX, CSS-in-JS, GraphQL and many other modern approaches to building sites.","date":"May 25, 2019","title":"Building a Static Gatsby-based Website with TypeScript"}}}]}},"pageContext":{}}}